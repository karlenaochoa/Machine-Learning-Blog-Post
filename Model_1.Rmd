---
title: "About Model 1"
description: |
  Learn more about our preliminary model
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)

library(tidyverse)
library(janitor)
library(rio)
library(magrittr)
library(here)
library(vip)
library(rsample)
library(baguette)
library(recipes)
library(tune)


full_train <- read.csv(here("data","train.csv")) %>%
                 select(-classification) %>% 
                 sample_frac(.10)

```

```{r include = FALSE}

frl <- import("https://nces.ed.gov/ccd/Data/zip/ccd_sch_033_1718_l_1a_083118.zip",
              setclass = "tbl_df")  %>% 
  janitor::clean_names()  %>% 
  filter(st == "OR")  %>%
  select(ncessch, lunch_program, student_count)  %>% 
  mutate(student_count = replace_na(student_count, 0))  %>% 
  pivot_wider(names_from = lunch_program,
              values_from = student_count)  %>% 
  janitor::clean_names()  %>% 
  mutate(ncessch = as.double(ncessch))

stu_counts <- import("https://github.com/datalorax/ach-gap-variability/raw/master/data/achievement-gaps-geocoded.csv",
                     setclass = "tbl_df")  %>% 
                     filter(state == "OR" & year == 1718)  %>% 
                     count(ncessch, wt = n)  %>% 
                     mutate(ncessch = as.double(ncessch))

frl <- left_join(frl, stu_counts)

frl <- frl  %>% 
    mutate(prop_free = (free_lunch_qualified/n),
           prop_reduce = reduced_price_lunch_qualified/n)

```

```{r include = FALSE}
data <- left_join(full_train, frl, join_by = ncessch)

head(data)
```

```{r include = FALSE}
set.seed(200)
splt <- initial_split(data)
train <- training(splt)
```


The purpose of preliminary fit 1 was to use basic feature engineering to make our initial predictions of test scores. We used a linear regression model with all variables in the data set (besides our outcome and ID variables). For this blog, we only sampled 10% of the data for computational efficiency. 

One of the biggest hurdles for learning and using machine learning (ML) is getting the right balance of bias-variance. The purpose of ML is to be able to predict unseen scores or data, so we use available data to model the best prediction. The bias reflects how well the model fits the structure of our data; low bias is good, where high bias ignores important details of our data. Variance reflects variability of our model predictions for a specific data point; flexible models are more likely to have higher variance and are prone to overfitting to the training data and not generalize well to new data. Ideally, our model would have low bias and low variance, but for our first model we will run a linear model which typically has a high bias and low variance. 

We used k-fold cross validation (CV) to split our training data into 10 distinct samples of data. Although k-fold may have more variability than other CV methods, it is the most widely used method.  For each fold, 10% of the training data are sampled for assessment, and the remaining of the training data serve as our analysis set for that fold. Our performance measure (RMSE) will be the average RMSE across all 10 folds.


The first thing we did was create a recipe (the regression equation that is usually defined inside of a lm call) that defined score as our dependent variable with all other variables predicting it. 

We assigned all school and id vars to "id_vars", this ignores them as predictors and does not apply any of the following steps to these variables. Next, we took care of missing data with step_unknown and step_novel and treated missing data with mean imputation. We also fixed the date format and removed predictors that had no variance, and dummy coded nominal variables. 

```{r}
rec <- recipe(score ~ ., train) %>% 
  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt)) %>%
  update_role(contains("id"), ncessch, new_role = "id vars") %>% 
  step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars"))  %>% 
  step_unknown(all_nominal()) %>% 
  step_novel(all_nominal()) %>%
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0)  %>% 
  step_dummy(all_nominal())  %>% 
  step_nzv(all_predictors())
  

rec
```

As the "Inputs" displays, we have 6 ID variables, 1 outcome, and 40 predictors. 

```{r include = FALSE}
prep(rec)

mod <- linear_reg() %>%
  set_engine("lm") %>% 
  set_mode("regression")

cv <- vfold_cv(train)
```


```{r inlcude = FALSE}
m1 <- mod %>% 
  fit_resamples(preprocessor = rec,
                resamples = cv,
                control = control_resamples(verbose = TRUE))

m1
```

Our initial model without feature engineering produced a mean rmse of 87.8. 

```{r}
m1 %>%  
  collect_metrics() %>% 
  filter(`.metric` == "rmse")
```



In our next model we aimed to produce a more predictive model by using step_interact to model the interaction between longitude and latitude because we thought geographic location of schools would influence childrenâ€™s test scores. We updated the recipe and used 10-fold CV again.

```{r}
rec2 <- recipe(score ~ ., train) %>% 
  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt)) %>%
  update_role(contains("id"), ncessch, new_role = "id vars") %>% 
  step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars"))  %>% 
  step_unknown(all_nominal()) %>% 
  step_novel(all_nominal()) %>%
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0)  %>% 
  step_dummy(all_nominal())  %>% 
  step_nzv(all_predictors())  %>% 
  step_interact(terms = ~ lat:lon) 
  

rec2
```

```{r include = FALSE}
m2 <- mod %>% 
  fit_resamples(preprocessor = rec2,
                resamples = cv,
                control = control_resamples(verbose = TRUE))

m2
```

This model produced the same average rmse as in model 1, 87.8. We decided to keep this as our final model. 

```{r}
m2 %>%  
  collect_metrics() %>% 
  filter(`.metric` == "rmse")
```

```{r include = FALSE}

train2 <- select(train, -contains("id"), -ncessch, -tst_dt)

vip_m2 <- vip(lm(score ~ lat:lon + tag_ed_fg + sp_ed_fg + ethnic_cd + econ_dsvntg + enrl_grd, train),mapping = aes(fill= Sign), num_features = 12)
              
```


To better understand the predictive accuracy of our model, we can use a package called VIP to look at a plot of variable importance on score. We choose to examine variables we thought would be theoretically important for test scores, such as grade in school, ethnicity, whether children participated in a special education program or talented and gifted program, or qualified for a free or reduced lunch program (economic disadvantage). We also included the interaction between longitude and latitude since we thought it would improve our model. 

```{r include = FALSE}
plot1 <- vip_m2 +
  theme_minimal() +
  labs(title = "Variable importance plot for model 1",
       y = "Variable importance")
```

```{r}
plot1
```

As seen in the plot, latitude and longitude do not seem to be a variable with a lot of importance in our model, and instead grade in school and whether a child participated in a special education program or talented and gifted program seemed to be the most important variables of the ones examined. The salmon color bars represent negative values, while teal bars represent positive values. 

This first model allowed us to get acquainted with ML and specifically using recipes. We started to use some feature engineering to make a more predictive model and will continue to do so in subsequent models. 
