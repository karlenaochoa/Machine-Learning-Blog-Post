---
title: "About Model 2"
description: |
  Learn more about our second model
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)

library(tidyverse)
library(tidymodels)
library(janitor)
library(rio)
library(magrittr)
library(doParallel)
library(tictoc)
library(kknn)
library(here)
library(vip)
library(rsample)


full_train <- read.csv(here("data","train.csv")) %>%
                 select(-classification) %>% 
                 sample_frac(.01)

```

```{r include = FALSE}

frl <- import("https://nces.ed.gov/ccd/Data/zip/ccd_sch_033_1718_l_1a_083118.zip",
              setclass = "tbl_df")  %>% 
  janitor::clean_names()  %>% 
  filter(st == "OR")  %>%
  select(ncessch, lunch_program, student_count)  %>% 
  mutate(student_count = replace_na(student_count, 0))  %>% 
  pivot_wider(names_from = lunch_program,
              values_from = student_count)  %>% 
  janitor::clean_names()  %>% 
  mutate(ncessch = as.double(ncessch))

stu_counts <- import("https://github.com/datalorax/ach-gap-variability/raw/master/data/achievement-gaps-geocoded.csv",
                     setclass = "tbl_df")  %>% 
                     filter(state == "OR" & year == 1718)  %>% 
                     count(ncessch, wt = n)  %>% 
                     mutate(ncessch = as.double(ncessch))

frl <- left_join(frl, stu_counts)

frl <- frl  %>% 
    mutate(prop_free = (free_lunch_qualified/n),
           prop_reduce = reduced_price_lunch_qualified/n)

```

```{r include = FALSE}
data <- left_join(full_train, frl, join_by = ncessch)

head(data)
```

```{r include = FALSE}
set.seed(200)
splt <- initial_split(data)
train <- training(splt)

```

For our second preliminary fit we decided to use a *K* Nearest Neighbor model to try to improve our predictions by exploring tuning some hyper-parameters as well as exploring the size of the grids. 

As we know, KNN has some extra challenges. KNN models store the training data to use it for prediction, when a new sample (test data) is given, uses the K training set points that are closest to the new sample. This implies a lot of inefficient computation. Which is a problem we faced with the kind of data we had. (Spoiler alert) It took forever to run in our whole data set. For while working and testing tuning different things, we had to just use a small portion of the data, and that is what I will do here, to explain our procedure. At the end I will show how are predictions worked/improved for the full data set - but I won't run it! 

One important step for KNN is that we need to ensure all of our predictors are in the same units, since a key aspect of this model is the distance between predictors. IE: we will scale our predictors. This will be seen when we build our recipe. 

First, there are a couple other steps we needed to take. Let's start with our first KNN Model:

## KNN Model

We first set our model, and specify the engine (KNN) and mode (regression). Secondly, we specified what parameters we wanted to tune. For this first model we decided to tune the parameters:
- # Neighbors (K) : this helps us control bias. Small K can become an over-fitting problem but a large K could mean we are under-fitting (too many irrelevant data points are being used)
- Weight: Kernel function that weights the distances between samples, ie: how we find the most similar nearest neighbors. 
- Distance: Parameter used when calculating the Minkowski distance (either Manhattan or Euclidean), ie: our measure of distance

Moreover, we'll use non-regular/random grids in our model. This means that the range for the possible values of our parameters will be defined , and the multidimensional space will be randomly sampled as many times necessary to cover an enough amount of the space. More specifically, we will use a space-filling design, which basically means that keeps the candidate values of our parameters in such way that they are away from one another but at the same time using well the entire parameter space. 

```{r def_model, include = TRUE}
knn_reg_mod <- nearest_neighbor() %>%
  set_engine("kknn") %>% 
  set_mode("regression") 

translate(knn_reg_mod)

knn_reg_mod <- knn_reg_mod %>% 
  set_args(neighbors = tune(),
                    weight_func = tune(),
                    dist_power = tune())

knn_params <- parameters(neighbors(range = c(15,70)), weight_func(), dist_power()) #specify parameters

knn_entr <- grid_max_entropy(knn_params, size = 20) #non-regular grid (sfd)

cv <- vfold_cv(train) #resample

parallel::detectCores()

```

Before fitting our model, we need to specify our recipe, taking into account that we are facing a regression problem. 

Preprocessing: 

As mentioned before, when specifying our data, we need to also work on our predictors a bit to make sure they are in the right conditions to be used in a KNN model. 
Here for example, we standarized our preditors using median impute and got rid of non-relevant predictors (ie: wiht zero variance). See comments in code. 

```{r recipe, include=TRUE}
rec3 <- recipe(score ~ ., train) %>% 
  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt)) %>%
  update_role(contains("id"), ncessch, new_role = "id vars") %>% 
  step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars"))  %>% 
  step_novel(all_nominal()) %>% #assigns previously unseen factor elvel to a new value
  step_unknown(all_nominal()) %>% #assigns missing value to unknown values
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0)  %>% 
  step_dummy(all_nominal())  %>% #dummy coded all nominl predictors
  step_nzv(all_predictors()) #remved predictors with zero vairance (non relevant)

rec3
```

Ok, now that we specified our model (tuning and grids) and that  we created a recipe, we are ready to fit our model! 

We will look at our top 5 parameters that give us the lowest rmse's


```{r fit_model, include = TRUE}

foreach::registerDoSEQ()
tic()


knn_reg_res3 <- tune::tune_grid(
  knn_reg_mod, #our model
  preprocessor = rec3, #specify recipe
  grid = knn_entr, #non-regular grid
  resamples = cv,
  control = tune::control_resamples(verbose = TRUE,
                                    save_pred = TRUE))

toc()

knn_reg_res3 %>% 
  show_best(metric = "rmse", n = 5) 


```

Now, let's take a look at a plot comparing different parameter values and their performance by looking at rmse and R-sq as our performance indicators. More specifically we will look at the performance of different values of distance and K , comparing for different weight functions.  (remember, we are only using a small portion of the data for this, for practical reasons.)

```{r plot, include= TRUE}
knn_reg_res3 %>% 
  autoplot() +
  geom_line()
```




Now, let's finalize our model (see comments in code for step by step): 

```{r finalize, include=TRUE}

foreach::registerDoSEQ()

# First, we select best tuning parameters
 knn_reg_best <- knn_reg_res3 %>%
  select_best(metric = "rmse")

# We finalize our model using the best tuning parameters
knn_reg_mod_final <- knn_reg_mod %>%
    finalize_model(knn_reg_best) 

 # We finalize our recipe using the best tuning parameters
 knn_reg_rec_final <- rec3 %>% 
   finalize_recipe(knn_reg_best)
 
  # Now we run our last fit on our initial data split


 knn_reg_test_results <- last_fit(knn_reg_mod_final, 
   preprocessor = knn_reg_rec_final, 
   split = splt)

 #Collect metrics
 knn_reg_test_results %>% 
   collect_metrics()

```


Now, we run it in our test data set, to get our predictions: 

```{r}
test <- testing(splt)

prepped_train <- knn_reg_rec_final  %>% 
  prep()  %>% 
  bake(train)  %>% 
  select(-contains("id"), -ncessch)

#real_test <- read_csv("../input/edld-654-spring-2020/test.csv",
                       #col_types = cols(.default = col_guess(), calc_admn_cd = col_character()))  %>% 
  #left_join(frl)

prepped_test <- knn_reg_rec_final   %>% 
  prep()  %>% 
  bake(test)%>%
  select(-contains("id"), -ncessch)


full_train_fit <- fit( knn_reg_mod_final, score ~ ., prepped_train)

preds <- predict(full_train_fit, new_data = prepped_test)
 pred_file <- tibble(Id = test$id, Predicted = preds$.pred) 


```

The results shown here are just for practical reasons - we are only using a small portion of the data, otherwise running and knitting the document would take too long (several, several hours). However, when we ran the model in the entire data for the kaggle competition, we obtained an rmse value of 89.32. 

Although we did see an improvement in the performance of our predictions, we also learnt that KNN is not ideal given the fact that it is very computationally inefficient, especially when dealing with a data set like ours. 
